{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add parent folder to the path\n",
    "module_path = str(Path.cwd().parents[0])\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader # load batches to the network\n",
    "\n",
    "# architectures\n",
    "from src.models import (\n",
    "    SimpleNet,\n",
    "    Rodan\n",
    ")\n",
    "\n",
    "# loss function and dataset loader\n",
    "from src.dataloader import DatasetONT # custom loader (used with DataLoader)\n",
    "from src.loss_functions import ctc_label_smoothing # custom CTC loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [batch size, channels, length]: torch.Size([2, 1, 4096])\n",
      "Output [batch size, length]: torch.Size([2, 271])\n",
      "CTC Loss\n",
      "Input CTC Loss [batch size] torch.Size([2])\n",
      "Target CTC Loss [batch size] torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "output_simplenet = 501\n",
    "dataset_example = DatasetONT(recfile=\"../data/subsample_val.hdf5\", output_network_len=output_simplenet)\n",
    "loader = iter(DataLoader(dataset_example, batch_size=2, shuffle=True))\n",
    "x,y, input_len, target_len = next(loader)\n",
    "print(\"Input [batch size, channels, length]:\" , x.shape)\n",
    "print(\"Output [batch size, length]:\" , y.shape)\n",
    "print(\"CTC Loss\")\n",
    "print(\"Input CTC Loss [batch size]\", input_len.shape)\n",
    "print(\"Target CTC Loss [batch size]\", target_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([501, 501]), tensor([271, 271]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len, target_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (conv1): Conv1d(1, 20, kernel_size=(20,), stride=(2,))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool1d(kernel_size=10, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(20, 50, kernel_size=(5,), stride=(1,))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool1d(kernel_size=10, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=50, out_features=5, bias=True)\n",
       "  (relu3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplenet = SimpleNet(n_channels = 1, n_classes = 271)\n",
    "simplenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 2, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = simplenet(x)\n",
    "output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "RODAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# DEFAULTCONFIG = dict(\n",
    "#     vocab=[\"<PAD>\", \"A\", \"C\", \"G\", \"T\"],\n",
    "#     activation_layer=\"mish\", # options: mish, swish, relu, gelu\n",
    "#     sqex_activation=\"mish\", # options: mish, swish, relu, gelu\n",
    "#     dropout=0.1,\n",
    "#     sqex_reduction=32\n",
    "# )\n",
    "\n",
    "# Config=namedtuple(\"CONFIG\",[\"vocab\", \"activation\", \"sqex_activation\", \"dropout\", \"sqex_reduction\"])\n",
    "\n",
    "# rodan = Rodan(config=Config(*DEFAULTCONFIG.values()))\n",
    "# rodan.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y, input_len, target_len = next(loader)\n",
    "# print(\"Input [batch size, channels, length]:\" , x.shape)\n",
    "# print(\"Input [batch size, length]:\" , y.shape)\n",
    "\n",
    "# output_rodan = rodan(x)\n",
    "# print(\"output rodan:\", output_rodan.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    training function\n",
    "    \"\"\"\n",
    "    size = len(dataloader) # number of datapoints in the dataset\n",
    "    model.train() # set model in training mode\n",
    "\n",
    "    for batch, (X,y, input_len, target_len) in enumerate(dataloader):\n",
    "        X, y, input_len, target_len = X.to(device), y.to(device), input_len.to(device), target_len.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y, input_lengths=input_len, target_lengths=target_len)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0: \n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, model_metadata=None):\n",
    "    \"\"\"\n",
    "    test function\n",
    "\n",
    "    Accuracy is measured here\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # let the model know that is in evaluation model\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X,y, input_len, target_len) in enumerate(dataloader):\n",
    "            X, y, input_len, target_len = X.to(device), y.to(device), input_len.to(device), target_len.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y, input_lengths=input_len, target_lengths=target_len).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "model = simplenet\n",
    "# simplenet params\n",
    "simplenet_output_len = 501\n",
    "rodan_output_len = 420 \n",
    "\n",
    "# params\n",
    "epochs = 5\n",
    "\n",
    "# dataset\n",
    "dataset_train = DatasetONT(recfile=\"../data/subsample_train.hdf5\", output_network_len=simplenet_output_len)\n",
    "dataset_val   = DatasetONT(recfile=\"../data/subsample_val.hdf5\", output_network_len=simplenet_output_len)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CTCLoss() #ctc_label_smoothing # \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CTCLoss() #ctc_label_smoothing # \n",
    "\n",
    "for batch, (X,y, input_len, target_len) in enumerate(dataloader_train):\n",
    "    X, y, input_len, target_len = X.to(device), y.to(device), input_len.to(device), target_len.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y, input_lengths=input_len, target_lengths=target_len)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([501, 16, 5]),\n",
       " torch.Size([16, 271]),\n",
       " tensor([561, 561, 561, 561, 561, 561, 561, 561, 561, 561, 561, 561, 561, 561,\n",
       "         561, 561]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, y.shape, input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(561)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.array(561))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:     nan [   16/   13]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (271) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     train(dataloader_train, model\u001b[39m=\u001b[39mmodel, loss_fn\u001b[39m=\u001b[39mloss_fn, optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[0;32m----> 5\u001b[0m     test(dataloader_val, model\u001b[39m=\u001b[39;49mmodel, loss_fn\u001b[39m=\u001b[39;49mloss_fn)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn, model_metadata)\u001b[0m\n\u001b[1;32m     16\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     17\u001b[0m         test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn(pred, y, input_lengths\u001b[39m=\u001b[39minput_len, target_lengths\u001b[39m=\u001b[39mtarget_len)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 18\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39;49margmax(\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m y)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m num_batches\n\u001b[1;32m     20\u001b[0m correct \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m size\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (271) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# run training \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader_train, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test(dataloader_val, model=model, loss_fn=loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4096])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rodan(x)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## basecalling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fast_ctc_decode import beam_search, viterbi_search\n",
    "alphabet = \"NACGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posteriors = output_rodan[:,1,:].detach().numpy()\n",
    "seq, path = viterbi_search(posteriors, alphabet)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posteriors = np.random.rand(100, len(alphabet)).astype(np.float32)\n",
    "posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, path = viterbi_search(posteriors, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GATCTCTATATGTGTATCACAGCAGTCATCTCATCGACGCACTCACT', 47)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq, len(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 32,\n",
       " 35,\n",
       " 37,\n",
       " 40,\n",
       " 41,\n",
       " 43,\n",
       " 46,\n",
       " 48,\n",
       " 50,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 56,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 68,\n",
       " 70,\n",
       " 72,\n",
       " 75,\n",
       " 78,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 88,\n",
       " 90,\n",
       " 91,\n",
       " 93,\n",
       " 94,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GATCTCTATATGTGTATCACAGCAGTCATCTCATCGACGCACTCACT', 47)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq, path = beam_search(posteriors, alphabet, beam_size=5, beam_cut_threshold=0.1)\n",
    "seq, len(seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
